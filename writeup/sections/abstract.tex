\begin{abstract}
	With a set of musical samples in ABC notation, we can generate music with a character-level LSTM. We can accomplish this task by feeding our LSTM musical characters in a sequence, and the network would slowly learn to predict the upcoming sequence of notes. Once trained, our model could generate music samples based on a short sequence of prompted notes. The style of the music generated by our final model has a rich, mature, and elaborate feel. By modifying the network depth, layer size, and dropout rate, we could tune our model to generate a range of musical samples. After hyperparameter tuning, we found that our LSTM created more elegant musical samples than a traditional recurrent neural network. The ability of the LSTM to drop information allows it to resemble the original test samples better than a standard RNN.
\end{abstract}
